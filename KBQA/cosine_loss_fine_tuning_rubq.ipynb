{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-12 18:41:41.567008: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-08-12 18:41:41.571134: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2022-08-12 18:41:41.571155: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "/root/.pyenv/versions/3.7.4/lib/python3.7/site-packages/pandas/compat/__init__.py:124: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import DistilBertModel, DistilBertTokenizer\n",
    "\n",
    "from wikidataintegrator import wdi_core\n",
    "from wikidata.client import Client\n",
    "import wikidata\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from IPython.core.debugger import set_trace\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#############################################################\n",
    "from utils import get_triplets_by_idd, get_description_name\n",
    "from datasets import load_rubq, load_simple_questions, combined_dataset_non_stochastic, expand_rubq, rubq_dataset, sq_dataset\n",
    "from models import EncoderBERT, get_projection_module_simple, get_tokenizer\n",
    "from reject import reject_by_metric\n",
    "from train import train_ensemble\n",
    "from eval_models import eval_ensemble\n",
    "from get_props import presearch_sq, presearch_rubq\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:6\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:6\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Graph Embeddings and Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#path to full list of embeddings and full list of ids (one2one correspondence with embeddings)\n",
    "PATH_TO_EMBEDDINGS_Q = \"../new_data/entitie_embeddings_ru.json\" \n",
    "PATH_TO_IDS = \"../new_data/entitie_ids_ru_filtered.json\"\n",
    "PATH_TO_EMBEDDINGS_P = \"../new_data/entitie_P_embeddings_ru.json\" \n",
    "\n",
    "graph_embeddings_Q = json.load(open(PATH_TO_EMBEDDINGS_Q))\n",
    "graph_embeddings_P = json.load(open(PATH_TO_EMBEDDINGS_P))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/notebook/meker/KBQA/datasets.py:91: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  answers_train = np.array(answers)[train_ids]\n",
      "/notebook/meker/KBQA/datasets.py:96: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  answers_val = np.array(answers)[val_ids]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "308\n",
      "296\n",
      "1186\n",
      "16414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 16414/16414 [00:00<00:00, 219656.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8327\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "\n",
    "MASTER_SEED = 42\n",
    "\n",
    "questions_train, relations_train, entities_train, answers_train, questions_val, relations_val, entities_val, answers_val, questions_test, answers_test = load_rubq(MASTER_SEED, graph_embeddings_Q, graph_embeddings_P)\n",
    "simple_questions_train, simple_questions_val = load_simple_questions(MASTER_SEED, graph_embeddings_Q, graph_embeddings_P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset and Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "\n",
    "expanded_questions_train, expanded_answers_train, expanded_relations_train, expanded_entities_train = expand_rubq(questions_train, answers_train, entities_train, relations_train)\n",
    "\n",
    "finetune_train_dataset = rubq_dataset(expanded_questions_train, expanded_answers_train, expanded_entities_train, expanded_relations_train, graph_embeddings_Q, graph_embeddings_P, device)\n",
    "finetune_val_dataset = rubq_dataset(questions_val, answers_val, entities_val, relations_val, graph_embeddings_Q, graph_embeddings_P, device)\n",
    "\n",
    "finetune_val_dataloader = torch.utils.data.DataLoader(finetune_val_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "train_dataset = sq_dataset(graph_embeddings_Q, graph_embeddings_P, simple_questions_train, device)\n",
    "val_dataset = sq_dataset(graph_embeddings_Q, graph_embeddings_P, simple_questions_val, device)\n",
    "\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_Q = graph_embeddings_Q\n",
    "ids_list = list(graph_embeddings_Q.keys())\n",
    "embeddings_Q = [embeddings_Q[Q] for Q in ids_list]\n",
    "embeddings_tensor_Q = torch.FloatTensor(embeddings_Q)\n",
    "\n",
    "embeddings_P = graph_embeddings_P\n",
    "embeddings_P = [embeddings_P[P] for P in graph_embeddings_P.keys()]\n",
    "embeddings_tensor_P = torch.FloatTensor(embeddings_P)\n",
    "\n",
    "candidates = list(np.load(\"./data/presearched_fixed_rubq_test.npy\", allow_pickle=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.CosineEmbeddingLoss()\n",
    "loss_name = str(loss)[:-2]\n",
    "proj_hidden_size = 512\n",
    "\n",
    "models_path = Path(f'./models/{loss_name}_{proj_hidden_size}_fine_tuning_rubq/')\n",
    "models_path.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 1000/1000 [00:00<00:00, 211236.10it/s]\n"
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "\n",
    "N_EPOCHS = 30\n",
    "N_MODELS = 5\n",
    "\n",
    "sq_val_cands = np.load('data/presearched_fixed_sq_val.npy', allow_pickle=True)\n",
    "rubq_val_cands = np.load('data/presearched_fixed_rubq_val.npy', allow_pickle=True)\n",
    "\n",
    "sq_questions_val = []\n",
    "sq_answers_val = []\n",
    "\n",
    "for e, p, a, q in tqdm(simple_questions_val):\n",
    "    sq_questions_val.append(q)\n",
    "    sq_answers_val.append([a])\n",
    "    \n",
    "sq_questions_val = np.array(sq_questions_val)\n",
    "sq_answers_val = np.array(sq_answers_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %autoreload\n",
    "\n",
    "# train_ensemble(N_MODELS, N_EPOCHS, proj_hidden_size, train_dataset, val_dataloader, models_path, device, loss, sq_val_cands, sq_questions_val, sq_answers_val, graph_embeddings_P, graph_embeddings_Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_losses = []\n",
    "# val_losses = []\n",
    "# val_accs = []\n",
    "\n",
    "# for i in range(N_MODELS):\n",
    "#     train_losses.append(np.load(models_path / f'train_loss_{i}.npy'))\n",
    "#     val_losses.append(np.load(models_path / f'val_loss_{i}.npy'))\n",
    "#     val_accs.append(np.load(models_path / f'val_acc_{i}.npy'))\n",
    "\n",
    "# mean_train_loss = np.mean(train_losses, axis=0)\n",
    "# std_train_loss = np.std(train_losses, axis=0)\n",
    "\n",
    "# mean_val_loss = np.mean(val_losses, axis=0)\n",
    "# std_val_loss = np.std(val_losses, axis=0)\n",
    "\n",
    "# mean_val_acc = np.mean(val_accs, axis=0)\n",
    "# std_val_acc = np.std(val_accs, axis=0)\n",
    "\n",
    "# plt.plot(list(range(N_EPOCHS)), mean_train_loss, label='Train loss')\n",
    "# plt.fill_between(list(range(N_EPOCHS)), mean_train_loss - std_train_loss, mean_train_loss + std_train_loss, alpha=0.4)\n",
    "# plt.plot(list(range(N_EPOCHS)), mean_val_loss, label='Val loss')\n",
    "# plt.fill_between(list(range(N_EPOCHS)), mean_val_loss - std_val_loss, mean_val_loss + std_val_loss, alpha=0.4)\n",
    "# plt.plot(list(range(N_EPOCHS)), mean_val_acc, label='Val acc')\n",
    "# plt.fill_between(list(range(N_EPOCHS)), mean_val_acc - std_val_acc, mean_val_acc + std_val_acc, alpha=0.4)\n",
    "\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Fine Tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "\n",
    "N_MODELS=5\n",
    "proj_hidden_size = 512\n",
    "\n",
    "models = []\n",
    "for i in range(N_MODELS):\n",
    "    encoder = EncoderBERT(device)\n",
    "    projection_E = get_projection_module_simple(device, proj_hidden_size)\n",
    "    projection_Q = get_projection_module_simple(device, proj_hidden_size)\n",
    "    projection_P = get_projection_module_simple(device, proj_hidden_size)\n",
    "\n",
    "    encoder.load_state_dict(torch.load(models_path / f'encoder_{i}_best_acc.pt'))\n",
    "    projection_E.load_state_dict(torch.load(models_path / f'projection_E_{i}_best_acc.pt'))\n",
    "    projection_Q.load_state_dict(torch.load(models_path / f'projection_Q_{i}_best_acc.pt'))\n",
    "    projection_P.load_state_dict(torch.load(models_path / f'projection_P_{i}_best_acc.pt'))\n",
    "    \n",
    "    models.append({'encoder': encoder, 'projection_P': projection_P, 'projection_Q': projection_Q, 'projection_E': projection_E})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N_EPOCHS = 20\n",
    "\n",
    "# train_ensemble(N_MODELS, N_EPOCHS, proj_hidden_size, finetune_train_dataset, finetune_val_dataloader, models_path, device, loss, rubq_val_cands, questions_val, answers_val, graph_embeddings_P, graph_embeddings_Q, models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_losses = []\n",
    "# val_losses = []\n",
    "# val_accs = []\n",
    "\n",
    "# for i in range(N_MODELS):\n",
    "#     train_losses.append(np.load(models_path / f'train_loss_{i}.npy'))\n",
    "#     val_losses.append(np.load(models_path / f'val_loss_{i}.npy'))\n",
    "#     val_accs.append(np.load(models_path / f'val_acc_{i}.npy'))\n",
    "\n",
    "# mean_train_loss = np.mean(train_losses, axis=0)\n",
    "# std_train_loss = np.std(train_losses, axis=0)\n",
    "\n",
    "# mean_val_loss = np.mean(val_losses, axis=0)\n",
    "# std_val_loss = np.std(val_losses, axis=0)\n",
    "\n",
    "# mean_val_acc = np.mean(val_accs, axis=0)\n",
    "# std_val_acc = np.std(val_accs, axis=0)\n",
    "\n",
    "# plt.plot(list(range(N_EPOCHS)), mean_train_loss, label='Train loss')\n",
    "# plt.fill_between(list(range(N_EPOCHS)), mean_train_loss - std_train_loss, mean_train_loss + std_train_loss, alpha=0.4)\n",
    "# plt.plot(list(range(N_EPOCHS)), mean_val_loss, label='Val loss')\n",
    "# plt.fill_between(list(range(N_EPOCHS)), mean_val_loss - std_val_loss, mean_val_loss + std_val_loss, alpha=0.4)\n",
    "# plt.plot(list(range(N_EPOCHS)), mean_val_acc, label='Val acc')\n",
    "# plt.fill_between(list(range(N_EPOCHS)), mean_val_acc - std_val_acc, mean_val_acc + std_val_acc, alpha=0.4)\n",
    "\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "entities = np.load('data/candidate_entities_sq_test.npy', allow_pickle=True)\n",
    "\n",
    "with open('data/entity_subgraphs_sq_test.pickle', 'rb') as handle:\n",
    "    entity_subgraphs = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sq_candidates = []\n",
    "\n",
    "for question_entities in entities:\n",
    "    candidates_dict = {}\n",
    "    for entity in list(question_entities.item()):\n",
    "        candidates_dict[entity] = entity_subgraphs[entity]\n",
    "    sq_candidates.append(candidates_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 4751/4751 [00:00<00:00, 150904.86it/s]\n"
     ]
    }
   ],
   "source": [
    "simple_questions_test = np.load(\"../new_data/simple_questions_test.npy\")\n",
    "\n",
    "simple_questions_filtered = []\n",
    "questions_sq = []\n",
    "answers_sq = []\n",
    "\n",
    "for e, p, a, q in tqdm(simple_questions_test):\n",
    "    if e in graph_embeddings_Q and a in graph_embeddings_Q and p in graph_embeddings_P:\n",
    "        simple_questions_filtered.append((e, p, a, q))\n",
    "        questions_sq.append(q)\n",
    "        answers_sq.append([a])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RuBQ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best acc models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/root/.pyenv/versions/3.7.4/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/notebook/meker/KBQA/models.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  q_ids = torch.tensor(questions_ids)\n",
      "40it [00:02, 14.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "\n",
    "N_MODELS=5\n",
    "proj_hidden_size = 512\n",
    "\n",
    "models = []\n",
    "for i in range(N_MODELS):\n",
    "    encoder = EncoderBERT(device)\n",
    "    projection_E = get_projection_module_simple(device, proj_hidden_size)\n",
    "    projection_Q = get_projection_module_simple(device, proj_hidden_size)\n",
    "    projection_P = get_projection_module_simple(device, proj_hidden_size)\n",
    "\n",
    "    encoder.load_state_dict(torch.load(models_path / f'encoder_{i}_best_acc.pt'))\n",
    "    projection_E.load_state_dict(torch.load(models_path / f'projection_E_{i}_best_acc.pt'))\n",
    "    projection_Q.load_state_dict(torch.load(models_path / f'projection_Q_{i}_best_acc.pt'))\n",
    "    projection_P.load_state_dict(torch.load(models_path / f'projection_P_{i}_best_acc.pt'))\n",
    "    models.append({'encoder': encoder, 'projection_P': projection_P, 'projection_Q': projection_Q, 'projection_E': projection_E})\n",
    "    \n",
    "val_cands = list(rubq_val_cands) + list(sq_val_cands)\n",
    "\n",
    "_, _, a_predicts, _, _, _, _, _, _, _, _, _, _, _, _, val_acc, _, _, _, _, _, a_model_predicts = eval_ensemble(questions_val, answers_val, graph_embeddings_P, graph_embeddings_Q, val_cands, models, device)\n",
    "\n",
    "models_corrects = [[], [], [], [], []]\n",
    "\n",
    "for i, (answers, models_preds, ensemble_preds) in enumerate(zip(answers_val, a_model_predicts, a_predicts)):\n",
    "    corr_models = 0\n",
    "    for j, model_preds in enumerate(models_preds):\n",
    "        if model_preds[0] in answers:\n",
    "            models_corrects[j].append(1.0)\n",
    "            corr_models += 1\n",
    "        else:\n",
    "            models_corrects[j].append(0.0)\n",
    "    \n",
    "    if corr_models > 2 and (ensemble_preds[0] not in answers):\n",
    "        print(i)\n",
    "    \n",
    "val_model_accs = [np.mean(model_corrects) for model_corrects in models_corrects]\n",
    "weights = val_model_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "1186it [01:09, 16.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.44435075885328834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "\n",
    "q_list, a_list, a_predicts, inv_ranks, top1_scores, top2_scores, e_stds, q_stds, p_stds, e_stds_norm, q_stds_norm, p_stds_norm, cosines_stds, entropies_of_mean, mean_entropies, acc, cosines_P_stds, cosines_Q_stds, cosines_E_stds, bad_question_ids, all_cosines, a_model_predicts = eval_ensemble(questions_test, answers_test, graph_embeddings_P, graph_embeddings_Q, candidates, models, device, ensembling_mode='average', weights=weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ind model accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.43929173693086004, 0.4266441821247892, 0.3827993254637437, 0.42074198988195616, 0.43929173693086004]\n",
      "0.439\n",
      "0.383\n",
      "0.422\n",
      "0.021\n"
     ]
    }
   ],
   "source": [
    "models_corrects = [[], [], [], [], []]\n",
    "\n",
    "for i, (answers, models_preds, ensemble_preds) in enumerate(zip(answers_test, a_model_predicts, a_predicts)):\n",
    "    corr_models = 0\n",
    "    if len(models_preds) == 0:\n",
    "        for model_corrects in models_corrects:\n",
    "            model_corrects.append(0.0)\n",
    "    for j, model_preds in enumerate(models_preds):\n",
    "        if model_preds[0] in answers:\n",
    "            models_corrects[j].append(1.0)\n",
    "            corr_models += 1\n",
    "        else:\n",
    "            models_corrects[j].append(0.0)\n",
    "    \n",
    "rubq_model_accs = [np.mean(model_corrects) for model_corrects in models_corrects]\n",
    "mean_acc = np.mean(rubq_model_accs)\n",
    "std_acc = np.std(rubq_model_accs)\n",
    "\n",
    "print(rubq_model_accs)\n",
    "print(round(max(rubq_model_accs), 3))\n",
    "print(round(min(rubq_model_accs), 3))\n",
    "print(round(mean_acc, 3))\n",
    "print(round(std_acc, 3)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "2438it [02:17, 17.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.49917965545529125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "\n",
    "q_list, a_list, a_predicts, inv_ranks, top1_scores, top2_scores, e_stds, q_stds, p_stds, e_stds_norm, q_stds_norm, p_stds_norm, cosines_stds, entropies_of_mean, mean_entropies, acc, cosines_P_stds, cosines_Q_stds, cosines_E_stds, bad_question_ids, all_cosines, a_model_predicts = eval_ensemble(questions_sq, answers_sq, graph_embeddings_P, graph_embeddings_Q, sq_candidates, models, device, ensembling_mode='average', weights=weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ind model accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.48728465955701394, 0.485233798195242, 0.49302707136997537, 0.485233798195242, 0.47703035274815425]\n",
      "0.493\n",
      "0.477\n",
      "0.486\n",
      "0.005\n"
     ]
    }
   ],
   "source": [
    "models_corrects = [[], [], [], [], []]\n",
    "\n",
    "for i, (answers, models_preds, ensemble_preds) in enumerate(zip(answers_sq, a_model_predicts, a_predicts)):\n",
    "    if len(models_preds) == 0:\n",
    "        for model_corrects in models_corrects:\n",
    "            model_corrects.append(0.0)\n",
    "            \n",
    "    for j, model_preds in enumerate(models_preds):\n",
    "        if model_preds[0] in answers:\n",
    "            models_corrects[j].append(1.0)\n",
    "            corr_models += 1\n",
    "        else:\n",
    "            models_corrects[j].append(0.0)\n",
    "    \n",
    "sq_model_accs = [np.mean(model_corrects) for model_corrects in models_corrects]\n",
    "mean_acc = np.mean(sq_model_accs)\n",
    "std_acc = np.std(sq_model_accs)\n",
    "\n",
    "print(sq_model_accs)\n",
    "print(round(max(sq_model_accs), 3))\n",
    "print(round(min(sq_model_accs), 3))\n",
    "print(round(mean_acc, 3))\n",
    "print(round(std_acc, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Epoch 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "40it [00:03, 13.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.325\n",
      "38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "\n",
    "N_MODELS=5\n",
    "proj_hidden_size = 512\n",
    "\n",
    "models = []\n",
    "for i in range(N_MODELS):\n",
    "    encoder = EncoderBERT(device)\n",
    "    projection_E = get_projection_module_simple(device, proj_hidden_size)\n",
    "    projection_Q = get_projection_module_simple(device, proj_hidden_size)\n",
    "    projection_P = get_projection_module_simple(device, proj_hidden_size)\n",
    "\n",
    "    encoder.load_state_dict(torch.load(models_path / f'encoder_{i}_20.pt'))\n",
    "    projection_E.load_state_dict(torch.load(models_path / f'projection_E_{i}_20.pt'))\n",
    "    projection_Q.load_state_dict(torch.load(models_path / f'projection_Q_{i}_20.pt'))\n",
    "    projection_P.load_state_dict(torch.load(models_path / f'projection_P_{i}_20.pt'))\n",
    "    models.append({'encoder': encoder, 'projection_P': projection_P, 'projection_Q': projection_Q, 'projection_E': projection_E})\n",
    "    \n",
    "_, _, a_predicts, _, _, _, _, _, _, _, _, _, _, _, _, val_acc, _, _, _, _, _, a_model_predicts = eval_ensemble(questions_val, answers_val, graph_embeddings_P, graph_embeddings_Q, val_cands, models, device)\n",
    "\n",
    "models_corrects = [[], [], [], [], []]\n",
    "\n",
    "for i, (answers, models_preds, ensemble_preds) in enumerate(zip(answers_val, a_model_predicts, a_predicts)):\n",
    "    corr_models = 0\n",
    "    for j, model_preds in enumerate(models_preds):\n",
    "        if model_preds[0] in answers:\n",
    "            models_corrects[j].append(1.0)\n",
    "            corr_models += 1\n",
    "        else:\n",
    "            models_corrects[j].append(0.0)\n",
    "    \n",
    "    if corr_models > 2 and (ensemble_preds[0] not in answers):\n",
    "        print(i)\n",
    "    \n",
    "val_model_accs = [np.mean(model_corrects) for model_corrects in models_corrects]\n",
    "weights = val_model_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "1186it [01:01, 19.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.4620573355817875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "\n",
    "q_list, a_list, a_predicts, inv_ranks, top1_scores, top2_scores, e_stds, q_stds, p_stds, e_stds_norm, q_stds_norm, p_stds_norm, cosines_stds, entropies_of_mean, mean_entropies, acc, cosines_P_stds, cosines_Q_stds, cosines_E_stds, bad_question_ids, all_cosines, a_model_predicts = eval_ensemble(questions_test, answers_test, graph_embeddings_P, graph_embeddings_Q, candidates, models, device, ensembling_mode='average', weights=weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ind model accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.44350758853288363, 0.4460370994940978, 0.43591905564924116, 0.43929173693086004, 0.44013490725126475]\n",
      "0.446\n",
      "0.436\n",
      "0.441\n",
      "0.003\n"
     ]
    }
   ],
   "source": [
    "models_corrects = [[], [], [], [], []]\n",
    "\n",
    "for i, (answers, models_preds, ensemble_preds) in enumerate(zip(answers_test, a_model_predicts, a_predicts)):\n",
    "    corr_models = 0\n",
    "    if len(models_preds) == 0:\n",
    "        for model_corrects in models_corrects:\n",
    "            model_corrects.append(0.0)\n",
    "    for j, model_preds in enumerate(models_preds):\n",
    "        if model_preds[0] in answers:\n",
    "            models_corrects[j].append(1.0)\n",
    "            corr_models += 1\n",
    "        else:\n",
    "            models_corrects[j].append(0.0)\n",
    "    \n",
    "rubq_model_accs = [np.mean(model_corrects) for model_corrects in models_corrects]\n",
    "mean_acc = np.mean(rubq_model_accs)\n",
    "std_acc = np.std(rubq_model_accs)\n",
    "\n",
    "print(rubq_model_accs)\n",
    "print(round(max(rubq_model_accs), 3))\n",
    "print(round(min(rubq_model_accs), 3))\n",
    "print(round(mean_acc, 3))\n",
    "print(round(std_acc, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "2438it [02:21, 17.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.4889253486464315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "\n",
    "q_list, a_list, a_predicts, inv_ranks, top1_scores, top2_scores, e_stds, q_stds, p_stds, e_stds_norm, q_stds_norm, p_stds_norm, cosines_stds, entropies_of_mean, mean_entropies, acc, cosines_P_stds, cosines_Q_stds, cosines_E_stds, bad_question_ids, all_cosines, a_model_predicts = eval_ensemble(questions_sq, answers_sq, graph_embeddings_P, graph_embeddings_Q, sq_candidates, models, device, ensembling_mode='average', weights=weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ind model accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.46882690730106646, 0.46595570139458575, 0.47333880229696473, 0.4815422477440525, 0.4696472518457752]\n",
      "0.482\n",
      "0.466\n",
      "0.472\n",
      "0.005\n"
     ]
    }
   ],
   "source": [
    "models_corrects = [[], [], [], [], []]\n",
    "\n",
    "for i, (answers, models_preds, ensemble_preds) in enumerate(zip(answers_sq, a_model_predicts, a_predicts)):\n",
    "    if len(models_preds) == 0:\n",
    "        for model_corrects in models_corrects:\n",
    "            model_corrects.append(0.0)\n",
    "            \n",
    "    for j, model_preds in enumerate(models_preds):\n",
    "        if model_preds[0] in answers:\n",
    "            models_corrects[j].append(1.0)\n",
    "            corr_models += 1\n",
    "        else:\n",
    "            models_corrects[j].append(0.0)\n",
    "    \n",
    "sq_model_accs = [np.mean(model_corrects) for model_corrects in models_corrects]\n",
    "mean_acc = np.mean(sq_model_accs)\n",
    "std_acc = np.std(sq_model_accs)\n",
    "\n",
    "print(sq_model_accs)\n",
    "print(round(max(sq_model_accs), 3))\n",
    "print(round(min(sq_model_accs), 3))\n",
    "print(round(mean_acc, 3))\n",
    "print(round(std_acc, 3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
