{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import DistilBertModel, DistilBertTokenizer\n",
    "\n",
    "from wikidataintegrator import wdi_core\n",
    "from wikidata.client import Client\n",
    "import wikidata\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#############################################################\n",
    "from utils import get_triplets_by_idd, get_description_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:5\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Graph Embeddings and Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path to full list of embeddings and full list of ids (one2one correspondence with embeddings)\n",
    "PATH_TO_EMBEDDINGS_Q = \"../new_data/entitie_embeddings_ru.json\" \n",
    "PATH_TO_IDS = \"../new_data/entitie_ids_ru_filtered.json\"\n",
    "PATH_TO_EMBEDDINGS_P = \"../new_data/entitie_P_embeddings_ru.json\" \n",
    "\n",
    "graph_embeddings_Q = json.load(open(PATH_TO_EMBEDDINGS_Q))\n",
    "graph_embeddings_P = json.load(open(PATH_TO_EMBEDDINGS_P))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "296\n"
     ]
    }
   ],
   "source": [
    "questions = list(np.load(\"../new_data/all_EN_rubq_val_questions_1_hop_uri.npy\"))\n",
    "relations = list(np.load(\"../new_data/all_rubq_val_relations_1_hop_uri.npy\"))\n",
    "entities = list(np.load(\"../new_data/all_rubq_val_entities_1_hop_uri.npy\"))\n",
    "answers = list(np.load(\"../new_data/all_rubq_val_answers_1_hop_uri.npy\",allow_pickle=True))\n",
    "\n",
    "questions_test = list(np.load(\"../new_data/all_EN_rubq_test_questions_1_hop_uri.npy\"))\n",
    "answers_test = list(np.load(\"../new_data/all_rubq_test_answers_1_hop_uri.npy\", allow_pickle=True))\n",
    "\n",
    "\n",
    "yes = []\n",
    "no = []\n",
    "no_ids = []\n",
    "\n",
    "for i, answer in enumerate(answers):\n",
    "    flag = True\n",
    "    for a in answer:\n",
    "        if not a in graph_embeddings_Q:\n",
    "            flag = False\n",
    "    if flag and relations[i] in graph_embeddings_P and entities[i] in graph_embeddings_Q:\n",
    "        yes.append(answer)\n",
    "    else:\n",
    "        no.append(answer)\n",
    "        no_ids.append(i)\n",
    "        \n",
    "for i in no_ids[::-1]:\n",
    "    del answers[i]\n",
    "    del questions[i]\n",
    "    del relations[i]\n",
    "    del entities[i]\n",
    "      \n",
    "        \n",
    "print(len(questions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_questions = np.load(\"../new_data/simple_questions_train.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16414/16414 [00:00<00:00, 219327.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8327\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "simple_questions_filtered = []\n",
    "print(len(simple_questions))\n",
    "for e, p, a, q in tqdm(simple_questions):\n",
    "    if e in graph_embeddings_Q and a in graph_embeddings_Q:\n",
    "        simple_questions_filtered.append((e, p, a, q))\n",
    "print(len(simple_questions_filtered))\n",
    "\n",
    "simple_questions = simple_questions_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mBert Text Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "model = BertModel.from_pretrained(\"bert-base-multilingual-cased\").to(device)\n",
    "\n",
    "class EncoderBERT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EncoderBERT,self).__init__()\n",
    "        self.encoder =  model\n",
    "    def forward(self,questions_ids):\n",
    "        q_ids = torch.tensor(questions_ids)\n",
    "        last_hidden_states = self.encoder(q_ids)[0]\n",
    "        q_emb = last_hidden_states.mean(1)\n",
    "        return q_emb\n",
    "    \n",
    "encoder = EncoderBERT().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainable projection modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "projection_E = nn.Sequential(\n",
    "    nn.Linear(768,512),\n",
    "    nn.ELU(),\n",
    "    nn.Linear(512,512),\n",
    "    nn.ELU(),\n",
    "    nn.Linear(512,200),\n",
    ").to(device).train()\n",
    "\n",
    "projection_Q = nn.Sequential(\n",
    "    nn.Linear(768,512),\n",
    "    nn.ELU(),\n",
    "    nn.Linear(512,512),\n",
    "    nn.ELU(),\n",
    "    nn.Linear(512,200),\n",
    ").to(device).train()\n",
    "\n",
    "projection_P = nn.Sequential(\n",
    "    nn.Linear(768,512),\n",
    "    nn.ELU(),\n",
    "    nn.Linear(512,512),\n",
    "    nn.ELU(),\n",
    "    nn.Linear(512,200),\n",
    ").to(device).train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset and Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "MAX_LEN_Q = 32\n",
    "class my_dataset_simple_questions(torch.utils.data.Dataset):\n",
    "    def __init__(self, simple_questions, graph_Q, graph_P):\n",
    "        self.simple_questions = simple_questions\n",
    "        self.graph_Q = graph_Q\n",
    "        self.graph_P = graph_P\n",
    "    def __len__(self):\n",
    "        return len(self.simple_questions)\n",
    "    def __getitem__ (self,i):\n",
    "        id_e, id_p, id_a, q = self.simple_questions[i]\n",
    "        input_ids = torch.tensor([tokenizer.encode(q, max_length=MAX_LEN_Q, add_special_tokens=True, pad_to_max_length=True)]).to(device)[0]\n",
    "        answer = id_a\n",
    "        if id_p[0] == \"P\":\n",
    "            relation = id_p\n",
    "        else:\n",
    "            relation = \"P\" + id_p[1:]\n",
    "        graph_Q_embedding = torch.FloatTensor(self.graph_Q[answer])\n",
    "        graph_P_embedding = torch.FloatTensor(self.graph_P[relation])\n",
    "        return (input_ids.to(device), graph_Q_embedding.to(device), graph_P_embedding.to(device))\n",
    "    \n",
    "class my_dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, questions, answers, relations, graph_Q, graph_P):\n",
    "        self.questions = questions\n",
    "        self.answers = answers\n",
    "        self.relations = relations\n",
    "        self.graph_Q = graph_Q\n",
    "        self.graph_P = graph_P\n",
    "    def __len__(self):\n",
    "        return len(self.questions)\n",
    "    def __getitem__ (self,i):\n",
    "        input_ids = torch.tensor([tokenizer.encode(self.questions[i], max_length=MAX_LEN_Q, add_special_tokens=True, pad_to_max_length=True)]).to(device)[0]\n",
    "        answer = self.answers[i]\n",
    "        relation = self.relations[i]\n",
    "        graph_Q_embedding = torch.FloatTensor(self.graph_Q[answer])\n",
    "        graph_P_embedding = torch.FloatTensor(self.graph_P[relation])\n",
    "        return (input_ids.to(device), graph_Q_embedding.to(device), graph_P_embedding.to(device))\n",
    "    \n",
    "class combined_dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, questions, answers, entities, relations, graph_Q, graph_P, simple_questions):\n",
    "        self.simple_questions = simple_questions\n",
    "        self.questions = questions\n",
    "        self.answers = answers\n",
    "        self.entities = entities\n",
    "        self.relations = relations\n",
    "        self.graph_Q = graph_Q\n",
    "        self.graph_P = graph_P\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.simple_questions)\n",
    "    def __getitem__ (self,i):\n",
    "        if i % 5 > 0:\n",
    "            id_e, id_p, id_a, q = self.simple_questions[i]\n",
    "            input_ids = torch.tensor([tokenizer.encode(q, max_length=MAX_LEN_Q, add_special_tokens=True, pad_to_max_length=True)]).to(device)[0]\n",
    "            answer = id_a\n",
    "            if id_p[0] == \"P\":\n",
    "                relation = id_p\n",
    "            else:\n",
    "                relation = \"P\" + id_p[1:]\n",
    "            graph_E_embedding = torch.FloatTensor(self.graph_Q[id_e])\n",
    "            graph_Q_embedding = torch.FloatTensor(self.graph_Q[answer])\n",
    "            graph_P_embedding = torch.FloatTensor(self.graph_P[relation])\n",
    "            return (input_ids.to(device), graph_E_embedding.to(device), graph_Q_embedding.to(device), graph_P_embedding.to(device))\n",
    "        else:\n",
    "            i = random.randint(0,len(self.questions) - 1)\n",
    "            input_ids = torch.tensor([tokenizer.encode(self.questions[i], max_length=MAX_LEN_Q, add_special_tokens=True, pad_to_max_length=True)]).to(device)[0]\n",
    "            entity = self.entities[i]\n",
    "            answer = self.answers[i]\n",
    "            ind = random.randint(0,len(answer) - 1)\n",
    "            answer = answer[ind]\n",
    "            relation = self.relations[i]\n",
    "            graph_E_embedding = torch.FloatTensor(self.graph_Q[entity])\n",
    "            graph_Q_embedding = torch.FloatTensor(self.graph_Q[answer])\n",
    "            graph_P_embedding = torch.FloatTensor(self.graph_P[relation])\n",
    "            return (input_ids.to(device), graph_E_embedding.to(device), graph_Q_embedding.to(device), graph_P_embedding.to(device))\n",
    "            \n",
    "    \n",
    "    \n",
    "train_dataset = combined_dataset(questions, answers, entities, relations, graph_embeddings_Q, graph_embeddings_P, simple_questions)\n",
    "train_batch_generator = torch.utils.data.DataLoader(train_dataset,batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.AdamW(\n",
    "    list(projection_Q.parameters()) + \\\n",
    "    list(projection_P.parameters()) + \\\n",
    "    list(projection_E.parameters()) + \\\n",
    "    list(encoder.parameters()), \n",
    "    lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/razzhigaev/anaconda3/envs/razzh37_nlp/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2110: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/home/razzhigaev/anaconda3/envs/razzh37_nlp/lib/python3.7/site-packages/ipykernel_launcher.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 0 0.17019693553447723\n",
      "EPOCH 1 0.16221800446510315\n",
      "EPOCH 2 0.16502806544303894\n",
      "EPOCH 3 0.16466915607452393\n",
      "EPOCH 4 0.21004383265972137\n",
      "EPOCH 5 0.12261909246444702\n",
      "EPOCH 6 0.17267999053001404\n",
      "EPOCH 7 0.08153051882982254\n",
      "EPOCH 8 0.19384366273880005\n",
      "EPOCH 9 0.09176598489284515\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 10\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    for X, y_e, y_q, y_p in train_batch_generator:\n",
    "        projection_Q.train()\n",
    "        projection_E.train()\n",
    "        projection_P.train()\n",
    "        \n",
    "        y_pred_e = projection_E(encoder(X))\n",
    "        y_pred_q = projection_Q(encoder(X))\n",
    "        y_pred_p = projection_P(encoder(X))\n",
    "        \n",
    "        loss = nn.MSELoss()(y_q,y_pred_q) + nn.MSELoss()(y_p,y_pred_p) + nn.MSELoss()(y_e,y_pred_e)\n",
    "        loss.backward()\n",
    "        \n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        \n",
    "    print(\"EPOCH\", epoch, loss.item())\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RuBQ 2.0 Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_Q = graph_embeddings_Q\n",
    "ids_list = list(graph_embeddings_Q.keys())\n",
    "embeddings_Q = [embeddings_Q[Q] for Q in ids_list]\n",
    "embeddings_tensor_Q = torch.FloatTensor(embeddings_Q)\n",
    "\n",
    "embeddings_P = graph_embeddings_P\n",
    "embeddings_P = [embeddings_P[P] for P in graph_embeddings_P.keys()]\n",
    "embeddings_tensor_P = torch.FloatTensor(embeddings_P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from natasha import (\n",
    "#     Segmenter,\n",
    "#     MorphVocab,\n",
    "    \n",
    "#     NewsEmbedding,\n",
    "#     NewsMorphTagger,\n",
    "#     NewsSyntaxParser,\n",
    "#     NewsNERTagger,\n",
    "    \n",
    "#     PER,\n",
    "#     NamesExtractor,\n",
    "\n",
    "#     Doc\n",
    "# )\n",
    "\n",
    "\n",
    "segmenter = Segmenter()\n",
    "morph_vocab = MorphVocab()\n",
    "\n",
    "emb = NewsEmbedding()\n",
    "morph_tagger = NewsMorphTagger(emb)\n",
    "syntax_parser = NewsSyntaxParser(emb)\n",
    "ner_tagger = NewsNERTagger(emb)\n",
    "\n",
    "# names_extractor = NamesExtractor(morph_vocab)\n",
    "\n",
    "# text = \"Кто автор поэмы 'энеида'?\"\n",
    "# doc = Doc(text)\n",
    "\n",
    "# doc.segment(segmenter)\n",
    "# doc.tag_morph(morph_tagger)\n",
    "# doc.tag_ner(ner_tagger)\n",
    "# for span in doc.spans:\n",
    "#     span.normalize(morph_vocab)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entity candidate selection fuctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ленинградская область\n"
     ]
    }
   ],
   "source": [
    "from natasha import (\n",
    "    Segmenter,\n",
    "    MorphVocab,\n",
    "    \n",
    "    NewsEmbedding,\n",
    "    NewsMorphTagger,\n",
    "    NewsSyntaxParser,\n",
    "    NewsNERTagger,\n",
    "    \n",
    "    PER,\n",
    "    NamesExtractor,\n",
    "\n",
    "    Doc)\n",
    "\n",
    "from natasha.doc import DocSpan, DocToken\n",
    "\n",
    "class Kostil_phrase_normalization():\n",
    "    def __init__(self):\n",
    "        self.segmenter = Segmenter()\n",
    "        self.morph_vocab = MorphVocab()\n",
    "\n",
    "        self.emb = NewsEmbedding()\n",
    "        self.morph_tagger = NewsMorphTagger(self.emb)\n",
    "        self.syntax_parser = NewsSyntaxParser(self.emb)\n",
    "        self.doc = None\n",
    "        \n",
    "        \n",
    "        \n",
    "    def phrase_preprocess(self, phrase):\n",
    "        self.doc = Doc(phrase)\n",
    "        self.doc.segment(self.segmenter)\n",
    "        self.doc.tag_morph(self.morph_tagger)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def get_tokens(self, phrase, tokens):\n",
    "        local_tokens = phrase.split()\n",
    "        result_tokens = []\n",
    "        for token in tokens:\n",
    "            if token.text in local_tokens:\n",
    "                result_tokens.append(token)\n",
    "        return result_tokens\n",
    "    \n",
    "    \n",
    "    def normalize(self, phrase):\n",
    "        self.phrase_preprocess(phrase)\n",
    "        \n",
    "        tokens = self.get_tokens(phrase, self.doc.tokens)\n",
    "        span = DocSpan('0', '2', type='LOC', text=phrase, tokens=tokens)\n",
    "        span.normalize(self.morph_vocab)\n",
    "        return span.normal\n",
    "\n",
    "\n",
    "\n",
    "normalizer = Kostil_phrase_normalization()\n",
    "phrase = 'Ленинградской области'\n",
    "print(normalizer.normalize(phrase))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"ru_core_news_sm\") \n",
    "\n",
    "def get_nouns(text):\n",
    "    text = text.replace('?', '')\n",
    "    doc = Doc(text)\n",
    "    doc.segment(segmenter)\n",
    "    doc.tag_morph(morph_tagger)\n",
    "    doc.tag_ner(ner_tagger)\n",
    "    for span in doc.spans:\n",
    "        span.normalize(morph_vocab)\n",
    "        \n",
    "    ents = [str(ent.normal) for ent in doc.spans]\n",
    "    \n",
    "    text1 = text.replace('\\'', '').replace('\\\"', '')\n",
    "    doc = Doc(text1)\n",
    "    doc.segment(segmenter)\n",
    "    doc.tag_morph(morph_tagger)\n",
    "    doc.tag_ner(ner_tagger)\n",
    "    for span in doc.spans:\n",
    "        span.normalize(morph_vocab)\n",
    "    \n",
    "    ents += [str(ent.normal) for ent in doc.spans]\n",
    "    \n",
    "    if '\"' in text:\n",
    "        text3 = text[text.find('\"')+1:]\n",
    "        text3 = text3[0:text3.find('\"')]\n",
    "        doc = Doc(text3)\n",
    "        doc.segment(segmenter)\n",
    "        doc.tag_morph(morph_tagger)\n",
    "        doc.tag_ner(ner_tagger)\n",
    "        for span in doc.spans:\n",
    "            span.normalize(morph_vocab)\n",
    "\n",
    "        ents += [str(ent.normal) for ent in doc.spans]\n",
    "        ents += [str(normalizer.normalize(text3))]\n",
    "        ents += [str(text3)]\n",
    "        \n",
    "    if '«' in text:\n",
    "        text4 = text[text.find('«')+1:]\n",
    "        text4 = text4[0:text4.find('»')]\n",
    "        doc = Doc(text4)\n",
    "        doc.segment(segmenter)\n",
    "        doc.tag_morph(morph_tagger)\n",
    "        doc.tag_ner(ner_tagger)\n",
    "        for span in doc.spans:\n",
    "            span.normalize(morph_vocab)\n",
    "\n",
    "        ents += [str(ent.normal) for ent in doc.spans]\n",
    "        ents += [str(normalizer.normalize(text4))]\n",
    "        ents += [str(text4)]\n",
    "        \n",
    "\n",
    "    if len(ents) == 0:\n",
    "        doc = nlp(text)\n",
    "        ents = [token.lemma_ for token in doc if token.pos_ == \"NOUN\" or token.pos_ == \"PROPN\"]\n",
    "        \n",
    "        bigrams = [normalizer.normalize(\" \".join(b)) for b in zip(text.split(\" \")[:-1], text.split(\" \")[1:])]\n",
    "        ents += bigrams\n",
    "    nouns_set = set(ents)\n",
    "    if \"\" in nouns_set:\n",
    "        nouns_set.remove(\"\")\n",
    "    return list(nouns_set)\n",
    "\n",
    "def get_top_ids_second_hop(text, first_hop_graph_E, second_hop_graph_Q, second_hop_graph_P, second_hop_ids_filtered_Q, second_hop_ids_filtered_P, topk):\n",
    "        projection_E.eval()\n",
    "        projection_P.eval()\n",
    "        projection_Q.eval()\n",
    "        bert_tokenize = lambda text: torch.tensor([tokenizer.encode(text, max_length=MAX_LEN_Q, add_special_tokens=True,pad_to_max_length=True)]).to(device)[0]\n",
    "        X = torch.tensor([tokenizer.encode(text, max_length=MAX_LEN_Q, add_special_tokens=True,pad_to_max_length=True)]).to(device)[0].to(device)\n",
    "        y_pred_e = projection_E(encoder(X[None,:]))\n",
    "        y_pred_q = projection_Q(encoder(X[None,:]))\n",
    "        y_pred_p = projection_P(encoder(X[None,:]))\n",
    "\n",
    "        embeddings_tensor_E = torch.FloatTensor(first_hop_graph_E)\n",
    "        embeddings_tensor_Q = torch.FloatTensor(second_hop_graph_Q)\n",
    "        embeddings_tensor_P = torch.FloatTensor(second_hop_graph_P)\n",
    "        \n",
    "        \n",
    "        cosines_descr_E = torch.cosine_similarity(embeddings_tensor_E.cpu(),y_pred_e.cpu())\n",
    "        cosines_descr_E = nn.Softmax()(cosines_descr_E)\n",
    "        \n",
    "        cosines_descr_Q = torch.cosine_similarity(embeddings_tensor_Q.cpu(),y_pred_q.cpu())\n",
    "        cosines_descr_Q = nn.Softmax()(cosines_descr_Q)\n",
    "\n",
    "        cosines_descr_P = torch.cosine_similarity(embeddings_tensor_P.cpu(),y_pred_p.cpu())\n",
    "        cosines_descr_P = nn.Softmax()(cosines_descr_P)\n",
    "        \n",
    "        cosines_aggr = cosines_descr_P * cosines_descr_Q * cosines_descr_E\n",
    "        inds = torch.topk(cosines_aggr,topk,sorted=True).indices.cpu().numpy()\n",
    "        return np.array(second_hop_ids_filtered_Q)[inds], cosines_aggr[inds]\n",
    "    \n",
    "\n",
    "def mp_get_second_hop_entities_by_idd(idd,d):\n",
    "    client = wikidata.client.Client()\n",
    "    entity = client.get(idd, load = True)\n",
    "    second_hop_qp = []\n",
    "    for x in tqdm(list(entity)): # Iterate over properties\n",
    "        prop = client.get(x.id, load = True)\n",
    "        try:\n",
    "            if type(prop) is wikidata.entity.Entity and type(entity[prop]) is wikidata.entity.Entity:\n",
    "                second_hop_qp.append((str(entity[prop].id),str(prop.id)))\n",
    "        except:\n",
    "            pass\n",
    "    d[idd] = second_hop_qp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$\n",
      "Processed questions: 3\n",
      "Accuracy:  0.6666666666666666\n",
      "MEAN MRR:  0.6666666666666666\n",
      "inv_ranks [0, 1.0, 1.0]\n",
      "#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$#$\n",
      "\n",
      "question:  What is the name of the capital of Romania?\n",
      "nouns:  ['capital of', 'the name', 'name of', 'is the', 'What is', 'of Romania', 'of the', 'the capital']\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-d34e1511127d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mids_q\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnoun\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnouns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mids_q\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mwdi_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWDItemEngine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_wd_search_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoun\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mMAX_PRESEARCH\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mids_q\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids_q\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids_q\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/razzh37_nlp/lib/python3.7/site-packages/wikidataintegrator/wdi_core.py\u001b[0m in \u001b[0;36mget_wd_search_results\u001b[0;34m(search_string, mediawiki_api_url, user_agent, max_results, language, dict_id_label, dict_id_all_info)\u001b[0m\n\u001b[1;32m    661\u001b[0m             \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'continue'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcont_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mcont_count\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    662\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 663\u001b[0;31m             \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmediawiki_api_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    664\u001b[0m             \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    665\u001b[0m             \u001b[0msearch_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/razzh37_nlp/lib/python3.7/site-packages/requests/api.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'allow_redirects'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'get'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/razzh37_nlp/lib/python3.7/site-packages/requests/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/razzh37_nlp/lib/python3.7/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    540\u001b[0m         }\n\u001b[1;32m    541\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/razzh37_nlp/lib/python3.7/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    653\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 655\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/razzh37_nlp/lib/python3.7/site-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    447\u001b[0m                     \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m                     \u001b[0mretries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_retries\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m                     \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m                 )\n\u001b[1;32m    451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/razzh37_nlp/lib/python3.7/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    704\u001b[0m                 \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m                 \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 706\u001b[0;31m                 \u001b[0mchunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunked\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    707\u001b[0m             )\n\u001b[1;32m    708\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/razzh37_nlp/lib/python3.7/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[0;31m# Trigger any extra validation we need to do.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m             \u001b[0;31m# Py2 raises this as a BaseSSLError, Py3 raises it as socket timeout.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/razzh37_nlp/lib/python3.7/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m   1008\u001b[0m         \u001b[0;31m# Force connect early to allow us to validate the connection.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1009\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"sock\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# AppEngine might not have  `.sock`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1010\u001b[0;31m             \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1011\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1012\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_verified\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/razzh37_nlp/lib/python3.7/site-packages/urllib3/connection.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    419\u001b[0m             \u001b[0mserver_hostname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mserver_hostname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m             \u001b[0mssl_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 421\u001b[0;31m             \u001b[0mtls_in_tls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtls_in_tls\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    422\u001b[0m         )\n\u001b[1;32m    423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/razzh37_nlp/lib/python3.7/site-packages/urllib3/util/ssl_.py\u001b[0m in \u001b[0;36mssl_wrap_socket\u001b[0;34m(sock, keyfile, certfile, cert_reqs, ca_certs, server_hostname, ssl_version, ciphers, ssl_context, ca_cert_dir, key_password, ca_cert_data, tls_in_tls)\u001b[0m\n\u001b[1;32m    448\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msend_sni\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m         ssl_sock = _ssl_wrap_socket_impl(\n\u001b[0;32m--> 450\u001b[0;31m             \u001b[0msock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtls_in_tls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mserver_hostname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mserver_hostname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    451\u001b[0m         )\n\u001b[1;32m    452\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/razzh37_nlp/lib/python3.7/site-packages/urllib3/util/ssl_.py\u001b[0m in \u001b[0;36m_ssl_wrap_socket_impl\u001b[0;34m(sock, ssl_context, tls_in_tls, server_hostname)\u001b[0m\n\u001b[1;32m    491\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mserver_hostname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mssl_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrap_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mserver_hostname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mserver_hostname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mssl_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrap_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/razzh37_nlp/lib/python3.7/ssl.py\u001b[0m in \u001b[0;36mwrap_socket\u001b[0;34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[0m\n\u001b[1;32m    421\u001b[0m             \u001b[0mserver_hostname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mserver_hostname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m             \u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m             \u001b[0msession\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m         )\n\u001b[1;32m    425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/razzh37_nlp/lib/python3.7/ssl.py\u001b[0m in \u001b[0;36m_create\u001b[0;34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001b[0m\n\u001b[1;32m    868\u001b[0m                         \u001b[0;31m# non-blocking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    869\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"do_handshake_on_connect should not be specified for non-blocking sockets\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 870\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_handshake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    871\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mOSError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/razzh37_nlp/lib/python3.7/ssl.py\u001b[0m in \u001b[0;36mdo_handshake\u001b[0;34m(self, block)\u001b[0m\n\u001b[1;32m   1137\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0.0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_handshake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#multiprocessing\n",
    "import multiprocessing\n",
    "from multiprocessing import Manager, Process\n",
    "\n",
    "MAX_PRESEARCH = 7\n",
    "q_list = []\n",
    "a_list = []\n",
    "a_predicts = []\n",
    "inv_ranks = []\n",
    "top1_scores = []\n",
    "\n",
    "\n",
    "for q, a in zip(questions_test, answers_test):\n",
    "    print(\"question: \", q)\n",
    "\n",
    "    nouns = get_nouns(str(q))\n",
    "    nouns = list(set(nouns))\n",
    "    print(\"nouns: \", nouns)\n",
    "\n",
    "    ids_q = []\n",
    "    for noun in nouns:\n",
    "        ids_q += wdi_core.WDItemEngine.get_wd_search_results(noun)[0:MAX_PRESEARCH]\n",
    "    ids_q = list(set(ids_q))\n",
    "    if len(ids_q) != 0:\n",
    "        second_hop_ids_QP = Manager().dict()\n",
    "        processes = []\n",
    "        for idd_q in ids_q:\n",
    "            processes.append(Process(target=mp_get_second_hop_entities_by_idd, args=(idd_q,second_hop_ids_QP)))\n",
    "        print(f\"# PROCESSES: {len(processes)}\")\n",
    "        for p in processes:\n",
    "            p.start()\n",
    "        for p in processes:\n",
    "            p.join()\n",
    "    \n",
    "    if len(second_hop_ids_QP) > 0:\n",
    "        first_hop_graph_E = []\n",
    "        second_hop_graph_Q = []\n",
    "        second_hop_ids_filtered_Q = []\n",
    "        second_hop_graph_P = []\n",
    "        second_hop_ids_filtered_P = []\n",
    "        for key in second_hop_ids_QP.keys():\n",
    "            for (idd_q, idd_p) in second_hop_ids_QP[key]:\n",
    "                if idd_q in graph_embeddings_Q and idd_p in graph_embeddings_P and key in graph_embeddings_Q:\n",
    "                    first_hop_graph_E.append(graph_embeddings_Q[key])\n",
    "                    second_hop_ids_filtered_Q.append(idd_q)\n",
    "                    second_hop_graph_Q.append(graph_embeddings_Q[idd_q])\n",
    "                    second_hop_ids_filtered_P.append(idd_p)\n",
    "                    second_hop_graph_P.append(graph_embeddings_P[idd_p])\n",
    "\n",
    "        if len(first_hop_graph_E) > 0:\n",
    "            predicts, scores = get_top_ids_second_hop(q, first_hop_graph_E, second_hop_graph_Q, second_hop_graph_P, second_hop_ids_filtered_Q, second_hop_ids_filtered_P, len(second_hop_graph_Q))\n",
    "            top1_scores.append(torch.max(scores).item())\n",
    "        else:\n",
    "            predicts = []\n",
    "            top1_scores.append(-1)\n",
    "    else:\n",
    "        predicts = []\n",
    "        top1_scores.append(-1)\n",
    "    \n",
    "    \n",
    "        \n",
    "        \n",
    "    print(\"Predicts: \",predicts)\n",
    "    a_predicts.append(predicts)\n",
    "    \n",
    "    clear_output(wait=True)\n",
    "    \n",
    "    inv_rs = []\n",
    "    if len(predicts) > 0:\n",
    "        for a_i in a:\n",
    "            if a_i not in predicts:\n",
    "                inv_r = 0\n",
    "            else:\n",
    "                inv_r = 1 / (list(predicts).index(a_i) + 1)\n",
    "            inv_rs.append(inv_r)\n",
    "        inv_ranks.append(max(inv_rs))\n",
    "    else:\n",
    "        inv_ranks.append(0)\n",
    "\n",
    "    print()\n",
    "    print(\"#$\"*30)\n",
    "    top1 = np.array(inv_ranks)[np.array(inv_ranks) == 1]\n",
    "    print(\"Processed questions:\", len(inv_ranks))\n",
    "    print(\"Accuracy: \", len(top1) / len(inv_ranks))\n",
    "    print(\"MEAN MRR: \", np.mean(inv_ranks))\n",
    "    print(\"inv_ranks\",[round(r,4) for r in inv_ranks])\n",
    "    print(\"#$\"*30)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_questions_test = np.load(\"/simple_questions_test.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_questions_filtered = []\n",
    "questions_sq = []\n",
    "answers_sq = []\n",
    "print(len(simple_questions))\n",
    "for e, p, a, q in tqdm(simple_questions_test):\n",
    "    if e in graph_embeddings_Q and a in graph_embeddings_Q and p in graph_embeddings_P:\n",
    "        simple_questions_filtered.append((e, p, a, q))\n",
    "        questions_sq.append(q)\n",
    "        answers_sq.append(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entity candidate selection functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\") \n",
    "\n",
    "def get_nouns(text):\n",
    "    text = text.replace('?', '')\n",
    "    \n",
    "    doc = nlp(text)\n",
    "    import spacy\n",
    "\n",
    "    lemmas = []\n",
    "    for token in doc:\n",
    "        lemmas.append(str(token.lemma_))\n",
    "    lemmatized_text = \" \".join(lemmas)\n",
    "        \n",
    "    ents = [str(ent) for ent in doc.ents]\n",
    "    if '\"' in text:\n",
    "        text3 = text[text.find('\"')+1:]\n",
    "        text3 = text3[0:text3.find('\"')]\n",
    "        ents += [str(text3)]\n",
    "        \n",
    "    if '«' in text:\n",
    "        text4 = text[text.find('«')+1:]\n",
    "        text4 = text4[0:text4.find('»')]\n",
    "        ents += [str(text4)]\n",
    "        \n",
    "\n",
    "    doc = nlp(text)\n",
    "    ents = [token.lemma_ for token in doc if token.pos_ == \"NOUN\" or token.pos_ == \"PROPN\"]\n",
    "\n",
    "    bigrams = [\" \".join(b) for b in zip(text.split(\" \")[:-1], text.split(\" \")[1:])]\n",
    "    ents += bigrams\n",
    "    bigrams = [\" \".join(b) for b in zip(lemmatized_text.split(\" \")[:-1], lemmatized_text.split(\" \")[1:])]\n",
    "    ents += bigrams\n",
    "\n",
    "    bigrams = [\" \".join(b) for b in zip(text.split(\" \")[:-2], text.split(\" \")[1:], text.split(\" \")[2:])]\n",
    "    ents += bigrams\n",
    "    bigrams = [\" \".join(b) for b in zip(lemmatized_text.split(\" \")[:-2], lemmatized_text.split(\" \")[1:], lemmatized_text.split(\" \")[2:])]\n",
    "    ents += bigrams\n",
    "        \n",
    "    nouns_set = set(ents)\n",
    "    if \"\" in nouns_set:\n",
    "        nouns_set.remove(\"\")\n",
    "    return list(nouns_set)\n",
    "\n",
    "\n",
    "def get_top_ids_second_hop(text, first_hop_graph_E, second_hop_graph_Q, second_hop_graph_P, second_hop_ids_filtered_Q, second_hop_ids_filtered_P, topk):\n",
    "        projection_E.eval()\n",
    "        projection_P.eval()\n",
    "        projection_Q.eval()\n",
    "        bert_tokenize = lambda text: torch.tensor([tokenizer.encode(text, max_length=MAX_LEN_Q, add_special_tokens=True,pad_to_max_length=True)]).to(device)[0]\n",
    "        X = torch.tensor([tokenizer.encode(text, max_length=MAX_LEN_Q, add_special_tokens=True,pad_to_max_length=True)]).to(device)[0].to(device)\n",
    "        y_pred_e = projection_E(encoder(X[None,:]))\n",
    "        y_pred_q = projection_Q(encoder(X[None,:]))\n",
    "        y_pred_p = projection_P(encoder(X[None,:]))\n",
    "\n",
    "        embeddings_tensor_E = torch.FloatTensor(first_hop_graph_E)\n",
    "        embeddings_tensor_Q = torch.FloatTensor(second_hop_graph_Q)\n",
    "        embeddings_tensor_P = torch.FloatTensor(second_hop_graph_P)\n",
    "        \n",
    "        cosines_descr_E = torch.cosine_similarity(embeddings_tensor_E.cpu(),y_pred_e.cpu())\n",
    "        cosines_descr_E = nn.Softmax()(cosines_descr_E)\n",
    "        \n",
    "        cosines_descr_Q = torch.cosine_similarity(embeddings_tensor_Q.cpu(),y_pred_q.cpu())\n",
    "        cosines_descr_Q = nn.Softmax()(cosines_descr_Q)\n",
    "\n",
    "        cosines_descr_P = torch.cosine_similarity(embeddings_tensor_P.cpu(),y_pred_p.cpu())\n",
    "        cosines_descr_P = nn.Softmax()(cosines_descr_P)\n",
    "\n",
    "        cosines_aggr = cosines_descr_P * cosines_descr_Q * cosines_descr_E\n",
    "        inds = torch.topk(cosines_aggr,topk,sorted=True).indices.cpu().numpy()\n",
    "        return np.array(second_hop_ids_filtered_Q)[inds], cosines_aggr[inds]\n",
    "    \n",
    "\n",
    "def mp_get_second_hop_entities_by_idd(idd,d):\n",
    "    client = wikidata.client.Client()\n",
    "    entity = client.get(idd, load = True)\n",
    "    second_hop_qp = []\n",
    "    for x in tqdm(list(entity)): # Iterate over properties\n",
    "        prop = client.get(x.id, load = True)\n",
    "        try:\n",
    "            if type(prop) is wikidata.entity.Entity and type(entity[prop]) is wikidata.entity.Entity:\n",
    "                second_hop_qp.append((str(entity[prop].id),str(prop.id)))\n",
    "        except:\n",
    "            pass\n",
    "    d[idd] = second_hop_qp\n",
    "\n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#multiprocessing\n",
    "import multiprocessing\n",
    "from multiprocessing import Manager, Process\n",
    "\n",
    "\n",
    "MAX_PRESEARCH = 3\n",
    "q_list = []\n",
    "a_list = []\n",
    "a_predicts = []\n",
    "inv_ranks = []\n",
    "top1_scores = []\n",
    "\n",
    "i = 0\n",
    "\n",
    "for q, a in zip(questions_sq, answers_sq):\n",
    "    print(\"question: \", q)\n",
    "    a = [a]\n",
    "\n",
    "    nouns = get_nouns(str(q))\n",
    "    nouns = list(set(nouns))\n",
    "    print(\"nouns: \", nouns)\n",
    "\n",
    "    ids_q = []\n",
    "    for noun in nouns:\n",
    "        ids_q += wdi_core.WDItemEngine.get_wd_search_results(noun)[0:MAX_PRESEARCH]\n",
    "    ids_q = list(set(ids_q))\n",
    "    if len(ids_q) != 0:\n",
    "        second_hop_ids_QP = Manager().dict()\n",
    "        processes = []\n",
    "        for idd_q in ids_q:\n",
    "            processes.append(Process(target=mp_get_second_hop_entities_by_idd, args=(idd_q,second_hop_ids_QP)))\n",
    "        print(f\"# PROCESSES: {len(processes)}\")\n",
    "        for p in processes:\n",
    "            p.start()\n",
    "        for p in processes:\n",
    "            p.join()\n",
    "\n",
    "    \n",
    "    if len(second_hop_ids_QP) > 0:\n",
    "        first_hop_graph_E = []\n",
    "        second_hop_graph_Q = []\n",
    "        second_hop_ids_filtered_Q = []\n",
    "        second_hop_graph_P = []\n",
    "        second_hop_ids_filtered_P = []\n",
    "        for key in second_hop_ids_QP.keys():\n",
    "            for (idd_q, idd_p) in second_hop_ids_QP[key]:\n",
    "                if idd_q in graph_embeddings_Q and idd_p in graph_embeddings_P and key in graph_embeddings_Q:\n",
    "                    first_hop_graph_E.append(graph_embeddings_Q[key])\n",
    "                    second_hop_ids_filtered_Q.append(idd_q)\n",
    "                    second_hop_graph_Q.append(graph_embeddings_Q[idd_q])\n",
    "                    second_hop_ids_filtered_P.append(idd_p)\n",
    "                    second_hop_graph_P.append(graph_embeddings_P[idd_p])\n",
    "\n",
    "        if len(first_hop_graph_E) > 0:\n",
    "            predicts, scores = get_top_ids_second_hop(q, first_hop_graph_E, second_hop_graph_Q, second_hop_graph_P, second_hop_ids_filtered_Q, second_hop_ids_filtered_P, len(second_hop_graph_Q))\n",
    "            top1_scores.append(torch.max(scores).item())\n",
    "        else:\n",
    "            predicts = []\n",
    "            top1_scores.append(-1)\n",
    "    else:\n",
    "        predicts = []\n",
    "        top1_scores.append(-1)\n",
    "    \n",
    "    \n",
    "        \n",
    "        \n",
    "    print(\"Predicts: \",predicts)\n",
    "    a_predicts.append(predicts)\n",
    "    \n",
    "    clear_output(wait=True)\n",
    "    \n",
    "    inv_rs = []\n",
    "    if len(predicts) > 0:\n",
    "        for a_i in a:\n",
    "            if a_i not in predicts:\n",
    "                inv_r = 0\n",
    "            else:\n",
    "                inv_r = 1 / (list(predicts).index(a_i) + 1)\n",
    "            inv_rs.append(inv_r)\n",
    "        inv_ranks.append(max(inv_rs))\n",
    "    else:\n",
    "        inv_ranks.append(0)\n",
    "\n",
    "    print()\n",
    "    print(\"#$\"*30)\n",
    "    top1 = np.array(inv_ranks)[np.array(inv_ranks) == 1]\n",
    "    print(\"Processed questions:\", len(inv_ranks))\n",
    "    print(f\"found: {sum(np.array(inv_ranks)>0)}\")\n",
    "    print(\"Accuracy: \", len(top1) / sum(np.array(inv_ranks)>0))\n",
    "    print(\"Absolute Accuracy: \", len(top1) / len(inv_ranks))\n",
    "    print(\"MEAN MRR: \", np.mean(inv_ranks))\n",
    "    print(\"inv_ranks\",[round(r,4) for r in inv_ranks])\n",
    "    print(\"#$\"*30)\n",
    "    print()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "razzh37_nlp",
   "language": "python",
   "name": "razzh37_nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
