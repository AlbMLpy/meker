{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-10 07:08:35.155315: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-08-10 07:08:35.159724: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2022-08-10 07:08:35.159744: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "/root/.pyenv/versions/3.7.4/lib/python3.7/site-packages/pandas/compat/__init__.py:124: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import DistilBertModel, DistilBertTokenizer\n",
    "\n",
    "from wikidataintegrator import wdi_core\n",
    "from wikidata.client import Client\n",
    "import wikidata\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from IPython.core.debugger import set_trace\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#############################################################\n",
    "from utils import get_triplets_by_idd, get_description_name\n",
    "from datasets import load_rubq, load_simple_questions, combined_dataset_non_stochastic\n",
    "from models import EncoderBERT, get_projection_module_simple, get_tokenizer\n",
    "from reject import reject_by_metric\n",
    "from train import train_ensemble\n",
    "from eval_models import eval_ensemble\n",
    "from get_props import presearch_sq, presearch_rubq\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:2\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset and Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path to full list of embeddings and full list of ids (one2one correspondence with embeddings)\n",
    "PATH_TO_EMBEDDINGS_Q = \"../new_data/entitie_embeddings_ru.json\" \n",
    "PATH_TO_IDS = \"../new_data/entitie_ids_ru_filtered.json\"\n",
    "PATH_TO_EMBEDDINGS_P = \"../new_data/entitie_P_embeddings_ru.json\" \n",
    "\n",
    "graph_embeddings_Q = json.load(open(PATH_TO_EMBEDDINGS_Q))\n",
    "graph_embeddings_P = json.load(open(PATH_TO_EMBEDDINGS_P))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/notebook/meker/KBQA/datasets.py:91: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  answers_train = np.array(answers)[train_ids]\n",
      "/notebook/meker/KBQA/datasets.py:96: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  answers_val = np.array(answers)[val_ids]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "308\n",
      "296\n",
      "1186\n",
      "16414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 16414/16414 [00:00<00:00, 214039.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8327\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "\n",
    "MASTER_SEED = 42\n",
    "\n",
    "questions_train, relations_train, entities_train, answers_train, questions_val, relations_val, entities_val, answers_val, questions_test, answers_test = load_rubq(MASTER_SEED, graph_embeddings_Q, graph_embeddings_P)\n",
    "simple_questions_train, simple_questions_val = load_simple_questions(MASTER_SEED, graph_embeddings_Q, graph_embeddings_P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rubq_candidates = list(np.load(\"./data/presearched_fixed_rubq_test.npy\", allow_pickle=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "entities = np.load('data/candidate_entities_sq_test.npy', allow_pickle=True)\n",
    "\n",
    "with open('data/entity_subgraphs_sq_test.pickle', 'rb') as handle:\n",
    "    entity_subgraphs = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sq_candidates = []\n",
    "\n",
    "for question_entities in entities:\n",
    "    candidates_dict = {}\n",
    "    for entity in list(question_entities.item()):\n",
    "        candidates_dict[entity] = entity_subgraphs[entity]\n",
    "    sq_candidates.append(candidates_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 4751/4751 [00:00<00:00, 230993.76it/s]\n"
     ]
    }
   ],
   "source": [
    "simple_questions_test = np.load(\"../new_data/simple_questions_test.npy\")\n",
    "\n",
    "simple_questions_filtered = []\n",
    "questions_sq = []\n",
    "answers_sq = []\n",
    "\n",
    "for e, p, a, q in tqdm(simple_questions_test):\n",
    "    if e in graph_embeddings_Q and a in graph_embeddings_Q and p in graph_embeddings_P:\n",
    "        simple_questions_filtered.append((e, p, a, q))\n",
    "        questions_sq.append(q)\n",
    "        answers_sq.append([a])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is top accuracy that model can achieve given this NER?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RuBQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "present = []\n",
    "for (answer, candidates) in zip(answers_test, rubq_candidates):\n",
    "    cand_answers = []\n",
    "    for ent in candidates.keys():\n",
    "        entity_candidates = [pair[0] for pair in candidates[ent]]\n",
    "        cand_answers.extend(entity_candidates)\n",
    "    \n",
    "    for cand_ans in cand_answers:\n",
    "        if cand_ans in answer:\n",
    "            present.append(1.0)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At least answer present in candidates:  0.7951096121416527\n"
     ]
    }
   ],
   "source": [
    "answer_present = len(present) / len(answers_test)\n",
    "print('At least answer present in candidates: ', answer_present)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "present = []\n",
    "for (answer, candidates) in zip(answers_sq, sq_candidates):\n",
    "    cand_answers = []\n",
    "    for ent in candidates.keys():\n",
    "        entity_candidates = [pair[0] for pair in candidates[ent]]\n",
    "        cand_answers.extend(entity_candidates)\n",
    "    \n",
    "    for cand_ans in cand_answers:\n",
    "        if cand_ans in answer:\n",
    "            present.append(1.0)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At least answer present in candidates:  0.6542247744052502\n"
     ]
    }
   ],
   "source": [
    "answer_present = len(present) / len(answers_sq)\n",
    "print('At least answer present in candidates: ', answer_present)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mGENRE + Stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named '_lzma'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_52057/2221140174.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mstanza\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mstanza\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'en'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.4/lib/python3.7/site-packages/stanza/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mstanza\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mstanza\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultilingual\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMultilingualPipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mstanza\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDocument\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mstanza\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresources\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mstanza\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresources\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minstallation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minstall_corenlp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload_corenlp_models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.4/lib/python3.7/site-packages/stanza/pipeline/core.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mstanza\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constants\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mstanza\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDocument\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mstanza\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfoundation_cache\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFoundationCache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mstanza\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mProcessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mProcessorRequirementsException\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mstanza\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregistry\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNAME_TO_PROCESSOR_CLASS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPIPELINE_NAMES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPROCESSOR_VARIANTS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.4/lib/python3.7/site-packages/stanza/models/common/foundation_cache.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mstanza\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbert_embedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mstanza\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpretrain\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPretrain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mlogger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetLogger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'stanza'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.4/lib/python3.7/site-packages/stanza/models/common/pretrain.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mlzma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.4/lib/python3.7/lzma.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0m_lzma\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m_lzma\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_encode_filter_properties\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_decode_filter_properties\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_compression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named '_lzma'"
     ]
    }
   ],
   "source": [
    "import stanza \n",
    "\n",
    "stanza.download('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stanza_nlp(text, device, language):\n",
    "    nlp = stanza.Pipeline(lang=language, processors='tokenize,ner', verbose= False, use_gpu = False)\n",
    "    doc = nlp(text)\n",
    "    return [ent.text for sent in doc.sentences for ent in sent.ents]\n",
    "\n",
    "def NER_Stanza(sentence, language, device=device):\n",
    "    res = stanza_nlp(text = sentence, device = device, language = language)\n",
    "    if res != []:\n",
    "        if len(res) == 1:\n",
    "            first_part, second_part = sentence.split(res[0])[0], sentence.split(res[0])[1]\n",
    "            output = first_part + \"[START] \" + res[0] + \" [END]\" + second_part\n",
    "            return output\n",
    "        else:\n",
    "            for i in range(len(res)):\n",
    "                output = ' '.join(['[START] {} [END]'.format(x) if x in res else x for x in sentence.split(\" \")])\n",
    "            return output\n",
    "                \n",
    "    else:\n",
    "        return sentence\n",
    "\n",
    "def UE_estimate(\n",
    "    data,\n",
    "    model,\n",
    "    ue_metrics = ['entropy', 'maxprob', 'delta', 'BALD' ,'expected entropy', 'predicted entropy'],\n",
    "    number_of_samples = 100,\n",
    "    beams = 5,\n",
    "    seed = 13,\n",
    "    task = \"Question Answering (object detection)\",\n",
    "    target_col = \"object\",\n",
    "    NER = None,\n",
    "    dataset = \"Simple Questions\",\n",
    "    language = \"en\"\n",
    "):\n",
    "    n = number_of_samples\n",
    "    rang = range(n)\n",
    "    df = data.sample(n = n, replace = False, random_state=seed)\n",
    "    \n",
    "    elif NER == \"Stanza\":\n",
    "        df = df.reset_index().drop(['index'], axis = 1)        \n",
    "        da = pd.DataFrame(df['question'].apply(lambda x: string.capwords(x)))\n",
    "        print(\"Started preparing text using NER\")\n",
    "        for i in tqdm(range(len(da))):\n",
    "            #print(\"before: \", da.loc[i, \"question\"])\n",
    "            #print(\"before df: \", df.loc[i, \"question\"])\n",
    "            da.loc[i, \"question\"] = NER_Stanza(da.loc[i, \"question\"], language)\n",
    "            \n",
    "            #print(\"after: \", da.loc[i, \"question\"])\n",
    "            #print(\"correct answer: \", df.loc[i, \"subject\"])\n",
    "        \n",
    "        df[\"question\"] = da[\"question\"]\n",
    "        print(\"Finished preparing text using NER\")\n",
    "        \n",
    "    print(\"Started sampling variants using mGENRE\")\n",
    "    model_mGENRE_mcdropout_result = model.sample(list(df['question']),\n",
    "                                                      beam = beams,\n",
    "                                                      prefix_allowed_tokens_fn=lambda batch_id, sent: [\n",
    "                                                          e for e in trie.get(sent.tolist())\n",
    "                                                          if e < len(model.task.target_dictionary)\n",
    "                                                      ],\n",
    "                                                      text_to_id=lambda x: max(lang_title2wikidataID[tuple(reversed(x.split(\" >> \")))], key=lambda y: int(y[1:])),\n",
    "                                                      marginalize=True,\n",
    "                                                      verbose = True,\n",
    "                                                      seed = seed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
